## 一 逻辑回归&交叉熵（重点）
LR真的是经久不衰啊，其实能问的地方好多好多，关于sigmoid函数其实可以从指数函数族的角度推导推导，然后就是交叉熵啦，最基本的就是从极大似然角度推导交叉熵。LR的基础推导看[这里](https://zhuanlan.zhihu.com/p/34325602) 。

####  1、简单介绍LR：
从指数函数族的角度和线性回归两个方面回答的：指数函数族方面就是先验的认为二分类问题服从伯努利分布，然后可以推出sigmoid函数(详见吴恩达cs229课件)；线性回归方面就是我们希望做分类的时候会给出一个阈值，超过就是1低于就是0，那就是个分段函数了，比较难以求解和优化，所以选了一个sigmoid函数作为二分类的近似。

逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的

 这里面其实包含了5个点 1：逻辑回归的假设，2：逻辑回归的损失函数，3：逻辑回归的求解方法，4：逻辑回归的目的，5:逻辑回归如何分类。这些问题是考核你对逻辑回归的基本了解。

#### 2、逻辑回归的基本假设

任何的模型都是有自己的假设，在这个假设下模型才是适用的。逻辑回归的第一个基本假设是假设数据服从伯努利分布。伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是p,抛中为负面的概率是1−p.在逻辑回归这个模型里面是假设 hθ(x) 为样本为正的概率，1−hθ(x)为样本为负的概率。那么整个模型可以描述为



$$
h_\theta\left(x;\theta \right )=p
$$


逻辑回归的第二个假设是假设样本为正的概率是 



$$
p=\frac{1}{1+e^{-\theta^{T} x}}
$$



所以逻辑回归的最终形式 
$$
h_\theta\left(x;\theta \right )=\frac{1}{1+e^{-\theta^{T} x}}
$$
#### 3、逻辑回归的损失函数


逻辑回归的损失函数是它的极大似然函数
$$
L_\theta\left(x\right )= \prod _{i=1}^{m}h_\theta(x^{i};\theta )^{y{i}}*(1-h_\theta(x^{i};\theta))^{1-y^{i}}
$$
###### LR 怎么优化？
优化方法，梯度下降，牛顿法

### 4、逻辑回归的求解方法

 - 由于该极大似然函数无法直接求解，我们一般通过对该函数进行梯度下降来不断逼急最优解。在这个地方其实会有个加分的项，考察你对其他优化方法的了解。因为就梯度下降本身来看的话就有随机梯度下降，批梯度下降，small batch 梯度下降三种方式，面试官可能会问这三种方式的优劣以及如何选择最合适的梯度下降方式。
     - 简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。
     - 随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。
小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。
 - 其实这里还有一个隐藏的更加深的加分项，看你了不了解诸如Adam，动量法等优化方法。因为上述方法其实还有两个致命的问题。
    - 第一个是如何对模型选择合适的学习率。自始至终保持同样的学习率其实不太合适。因为一开始参数刚刚开始学习的时候，此时的参数和最优解隔的比较远，需要保持一个较大的学习率尽快逼近最优解。但是学习到后面的时候，参数和最优解已经隔的比较近了，你还保持最初的学习率，容易越过最优点，在最优点附近来回振荡，通俗一点说，就很容易学过头了，跑偏了。
    - 第二个是如何对参数选择合适的学习率。在实践中，对每个参数都保持的同样的学习率也是很不合理的。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。

### 5、逻辑回归的目的
该函数的目的便是将数据二分类，提高准确率。

### 6、逻辑回归如何分类
逻辑回归作为一个回归(也就是y值是连续的)，如何应用到分类上去呢。y值确实是一个连续的变量。逻辑回归的做法是划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。

### 7.1、逻辑回归为什么使用sigmoid函数？
A:指数分布族
B:广义线性模型


### 7.2、逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？

 - 损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。至于原因大家可以求出这个式子的梯度更新

$$
\theta _j=\theta _j-\left ( y^{i} -h_\theta (x^{i};\theta ) \right )\ast x^{i}_j
$$

这个式子的更新速度只和$x^{i}_j$，$y^{i}​$相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。
 - 因为我们想要让每一个样本的预测都要得到最大的概率，即将所有的样本预测后的概率进行相乘都最大，也就是极大似然函数。
 - 为什么不选平方损失函数的呢？其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。
### 8、 逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？
先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。
但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。
如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

### 9、为什么我们还是会在训练的过程当中将高度相关的特征去掉？
去掉高度相关的特征会让模型的可解释性更好
可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。
### 10、逻辑回归的优缺点总结
在这里我们总结了逻辑回归应用到工业界当中一些优点：

 - 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
 - 模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
 - 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
 - 资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。
 - 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。

但是逻辑回归本身也有许多的缺点:

 - 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
 - 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。
 - 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。

### 11 离散化LR的好处
在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：

 - 稀疏向量内积乘法运算速度快，计算结果方便存储，容易scalable（扩展）。

 - 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。
 - 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。

 - 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。

 - 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。


## 二 GBDT系列（重点）
机器学习的话，gbdt肯定是重中之重了，跟LR一样是最最最常见的问题，工业界应用也很多，打比赛就不用说，xgb lgb是历届kaggle的两把快刀。

其实gbdt没有太多可说的，无非是boosting思想啊，为啥用负梯度啊之类。重点是在lgb和xgb对传统gbdt的优化上,[总结](http://djjowfy.com/2017/08/01/XGBoost%E7%9A%84%E5%8E%9F%E7%90%86/)。
## 三 SVM
[推导](https://www.zhihu.com/question/21094489)
## 四 RF&bagging
随机森林本身没啥东西，不过bagging的思想倒是非常有用，比赛中常用的降低过拟合方案，简单来说就是选xgb中偏差低方差高的模型，做bagging，效果非常显著。再有一点就是RF可以用oob做特征选择，效果也不错。


## 五 决策树
怎么说呢，问的也比较少了，就是三种树，ID3，C4.5，CART，区别和剪枝记一记。
#### 1、简单介绍决策树
ID3，C4.5，CART，分别用信息增益，信息增益比和gini系数作为分裂节点的依据

#### 2、ID3 怎么做特征的离散化的

#### 3、预剪枝和后剪枝

 预剪枝


 - 预剪枝就是在完全正确分类训练集之前，较早地停止树的生长。 具体在什么时候停止决策树的生长有多种不同的方法:
(1) 一种最为简单的方法就是在决策树到达一定高度的情况下就停止树的生长。
(2) 到达此结点的实例具有相同的特征向量，而不必一定属于同一类， 也可停止生长。
(3) 到达此结点的实例个数小于某一个阈值也可停止树的生长。
(4) 还有一种更为普遍的做法是计算每次扩张对系统性能的增益，如果这个增益值小于某个阈值则不进行扩展。
 - 优点&缺点
   - 由于预剪枝不必生成整棵决策树，且算法相对简单，效率很高， 适合解决大规模问题。但是尽管这一方法看起来很直接，但是 怎样精确地估计何时停止树的增长是相当困难的。
   - 预剪枝有一个缺点， 即视野效果问题 。 也就是说在相同的标准下，也许当前的扩展会造成过度拟合训练数据，但是更进一步的扩展能够满足要求，也有可能准确地拟合训练数据。这将使得算法过早地停止决策树的构造。

后剪枝
后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。

主要有四种：

 - REP-错误率降低剪枝
 - PEP-悲观剪枝
 - CCP-代价复杂度剪枝
 - MEP-最小错误剪枝

## 六 特征选择方法（重点）
特征选择是模型预处理的重要部分。方法很多：方差、相关系数、卡方检验、互信息、递归特征消除、基于惩罚的方法、树模型方法、单特征AUC、IV。

## 七 采样方法
主要有过采样和欠采样。

过采样：Smote方法及各种变种

欠采样：ensemble、nearMiss、Tomeklink、ENN

还有复杂分布的采样会用到MCMC。

## 八 聚类方法
k-means、k-means++、meanshift、DBSCAN、EM聚类、层次聚类。

## 九 评估指标（重点）
精准率、召回率、ACC、AUC、F1、KS、熵系列、信息增益、CTR、CVR、MSE系列。其中AUC是重点中的重点，被问了好多次，而且很细节，包括本质意义、计算方法等等，注意AUC是有两种计算方法的，[这里](https://www.cnblogs.com/peizhe123/p/5081559.html)有介绍。

## 十 过拟合（重点）
起因基本不太会考，解决方法就多了，降低模型复杂程度啊，dropout啊、bagging、正则、earlystop，数据增强、交叉验证。Dropout本质也是个bagging。

## 十一 batch normalization
[总结](https://www.zhihu.com/question/38102762/answer/302841181)

## 十二 梯度弥散/爆炸，怎么解决
改激活函数啊，BN啊，想lstm一样把*变+啊，加恒等映射的跳跃层啊，都可以。没有太好的文章，看看[这篇](https://zhuanlan.zhihu.com/p/28124810)讲resnet的吧。



## 十三 激活函数，比较

sigmod tanh relu maxout... 好多，这个随便一搜就一堆，放一个不太切题的文章吧，[回答。](https://www.zhihu.com/question/61265076/answer/186347780)

## 十四 优化方法（重点）

这就很多了，梯度下降系列、牛顿法系列，还有传统的模拟退火、遗传算法。牛顿法这回问的很多，不知道为啥。lan大神的花书讲的就很好，梯度下降的可以看[这个](https://blog.csdn.net/tsyccnh/article/details/76270707) 。这里要注意，有些面试官会让你实操，就比如给你一个方程，让你用梯度下降求解。

#### 1、谈谈牛顿法
收敛速度快，指数级收敛，但是不保证线性收敛，只保证二阶导收敛
#### 2、牛顿法怎么优化
记得看到过，忘了，没答上来
Cross Encropy是怎么推导出来的
没答上来，只说了熵怎么求，GG

## 十五 各种网络结构&模型（重点）

这个就太多了，CNN RNN就一堆，推荐的也是一堆，基本的DNN CNN RNN的forward和backprob都要熟悉，然后lstm、gru、attention也要会，还有各种encoder-decoder结构，这个就看积累了。 推荐部分有自己的一些模型，比如FM系列，lookalike、协同过滤之类的非深度学习模型，后面的W&D为首的融合模型也是搭积木。

