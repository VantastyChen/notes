# 数学基础

## 1、概率与似然
- 先验概率，条件概率与后验概率

先验概率是基于背景常识或者历史数据的统计得出的预判概率，一般只包含一个变量，例如$P(X)$，$P(Y)$。
条件概率是表示一个事件发生后另一个事件发生的概率，例如$P(Y|X)$代表X事件发生后Y事件发生的概率。
后验概率是由果求因，也就是在知道结果的情况下求原因的概率，例如Y事件是X引起的，那么$P(X|Y)$就是后验概率，也可以说它是事件发生后的反向条件概率。

- 似然函数
- 
在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。似然函数可以理解为条件概率的逆反。

在已知某个参数$\alpha$时，事件A会发生的条件概率可以写作$P(A;\alpha)$，也就是$P(A|\alpha)$。我们也可以构造似然性的方法来表示事件A发生后估计参数\alpha的可能性，也就表示为$L(\alpha|A)$，其中$L(\alpha|A)=P(A|\alpha)$。

这里Wikipedia的解释比较全面详细，可以参见[似然函数](https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%25E4%25BC%25BC%25E7%2584%25B6%25E5%2587%25BD%25E6%2595%25B0)。

-  最大似然估计（MLE）与最大后验概率（MAP）

最大似然估计是似然函数最初也是最自然的应用。似然函数取得最大值表示相应的参数能够使得统计模型最为合理。从这样一个想法出发，最大似然估计的做法是：首先选取似然函数（一般是概率密度函数或概率质量函数），整理之后求最大值。实际应用中一般会取似然函数的对数作为求最大值的函数，这样求出的最大值和直接求最大值得到的结果是相同的。似然函数的最大值不一定唯一，也不一定存在。

这里简单的说一下最大后验概率（MAP），如下面的公式

$$P(\alpha|X)=\frac{P(X|\alpha)P(\alpha)}{P(X)}$$

其中等式左边$P(\alpha|X)$表示的就是后验概率，优化目标即为$argmax_{\alpha}P(\alpha|X)$，即给定了观测值X以后使模型参数$\alpha$出现的概率最大。等式右边的分子式$P(X|\alpha)$即为似然函数$L(\alpha|X)$，MAP考虑了模型参数$\alpha$出现的先验概率$P(\alpha)$。即就算似然概率$P(X|\alpha)$很大，但是$\alpha$出现的可能性很小，也更倾向于不考虑模型参数为$\alpha$。
- 生成式模型与判别式模型

最后简单说一下生成式模型与判别式模型。
判别式模型学习的目标是条件概率$P(Y|X)$或者是决策函数$Y=f(X)$，其实这两者本质上是相同的。例如KNN，Decision Tree，SVM，CRF等模型都是判别式模型。
生成式模型学习的是联合概率分布$P(X,Y)$，从而求得条件概率分布$P(Y|X)$。例如NB，HMM等模型都是生成式模型。

## 2、L1不可导的时候该怎么办
当损失函数不可导,梯度下降不再有效,可以使用坐标轴下降法,梯度下降是沿着当前点的负梯度方向进行参数更新,而坐标轴下降法是沿着坐标轴的方向,假设有m个特征个数,坐标轴下降法进参数更新的时候,先固定m-1个值,然后再求另外一个的局部最优解,从而避免损失函数不可导问题。
使用Proximal Algorithm对L1进行求解,此方法是去优化损失函数上界结果。

## 3、sigmoid函数特性
定义域为![定义域](https://uploadfiles.nowcoder.com/images/20190315/311436_1552619092213_171B20A604B88BE2CC570EF936359F57)
值域为(-1,1)

函数在定义域内为连续和光滑的函数

处处可导,导数为![倒数](https://uploadfiles.nowcoder.com/images/20190315/311436_1552619110289_2FC0EF550E43398C033CAF8AE5CD23F6)

## 4、独立分布随机变量X1、X2服从N(0, 1)，求max(X1, X2)的期望？
由
![max](https://www.zhihu.com/equation?tex=max%5C%7BX_1%2CX_2%5C%7D%3D%5Cfrac%7BX_1+%2B+X_2+%2B%7CX_1-X_2%7C%7D%7B2%7D)
易见
![E(max)](https://www.zhihu.com/equation?tex=%5Ctext%7BE%7D%28max%5C%7BX_1%2CX_2%5C%7D%29+%3D+0+%2B+0+%2B+%5Cfrac%7B1%7D%7B2%7D%5Ctext%7BE%7D%7CX_1-X_2%7C)
记![y](https://www.zhihu.com/equation?tex=Y+%3D+X_1+-X_2)，注意到
![Y-N](https://www.zhihu.com/equation?tex=Y+%5Csim+N%280%2C+2%29)，
从而
![E|Y|](https://www.zhihu.com/equation?tex=%5Ctext%7BE%7D%7CY%7C+%3D+%5Cint_%7B-%5Cinfty%7D%5E%7B%2B%5Cinfty%7D%7Cy%7C%5Cfrac%7B1%7D%7B%5Csqrt%7B2+%5Cpi+%5Ccdot+2%7D%7De%5E%7B-%5Cfrac%7By%5E2%7D%7B2%5Ccdot2%7D%7D+dy+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B%5Cpi%7D%7D%5Cint_%7B0%7D%5E%7B%2B%5Cinfty%7Dye%5E%7B-%5Cfrac%7By%5E2%7D%7B4%7D%7D+dy+%3D+%5Cfrac%7B2%7D%7B%5Csqrt%7B%5Cpi%7D%7D)
综上
![E(max{})](https://www.zhihu.com/equation?tex=%5Ctext%7BE%7D%28max%5C%7BX_1%2CX_2%5C%7D%29+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B%5Cpi%7D%7D.)